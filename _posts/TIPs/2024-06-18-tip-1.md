---
layout: post
title: tip-1
topic: "TIP-1: CRO snapshot algorithm using threshold logical clock"
published: true
---

### Authors
Thomas Hsueh ([@guiltgyoza](https://github.com/guiltygyoza))

## Table of contents
1. [Abstract](#1-abstract)
2. [Motivation](#2-motivation)
3. [System model](#3-system-model)
4. [A good CRO snapshot?](#4-a-good-cro-snapshot)
5. [Problem definition](#5-problem-definition)
6. [A leader-based algorithm](#6-a-leader-based-algorithm)
7. [Issues](#7-issues)
8. [Threshold Logical Clock](#8-threshold-logical-clock)
9. [The TLC contract](#9-the-tlc-contract)
10. [TLC-based consensus](#10-tlc-based-consensus)
11. [A leaderless snapshot algorithm](#11-a-leaderless-snapshot-algorithm)
12. [Issues](#12-issues)
13. [Next](#13-next)
14. [Reference](#14-reference)

## 1. Abstract
This TIP proposes a snapshot algorithm for CROs, which uses a asychronous consensus algorithm based on threshold logical clock.

## 2. Motivation
In distributed systems, a global snapshot (or simply, snapshot) is the capturing of the states of all processes and all inflight messages between processes at an instant of time. It is important that taking a global snapshot does not require halting the distributed system.

At any given moment, a running CRO can have multiple replicas progressing at the same time. When the rate of individual replica making progress is higher than the rate at which replicas synchronize among themselves, replicas temporarily diverge. We can assume that a running CRO can always have diverging replicas, or diverging "versions" of itself. A CRO snapshot, then, is defined as the squashing of its multiple versions into a single representative version.

A CRO snapshot is useful in at least 3 ways:
1. A snapshot can serve as a single "state save" in time for the CRO to be persisted somewhere.
2. A snapshot can be used to determine which part of the CRO history (append-only log) is safe to be prune (compacted) away. State pruning or compaction is needed to solve the unbounded memory problem that plagues append-only logs and CRDTs. To bound memory usage, it is helpful to perform snapshot periodically.
3. A snapshot can be used to generate transactions to be submitted to a blockchain.

We need to devise a snapshot algorithm for CROs.

## 3. System model
Following [paper draft](https://github.com/topology-foundation/paper) v7.5 (and replacing "live objects" with "CROs"), we define the following system model:
1. A CRO's history is maintained in its **hash graph**, a directed acyclic graph (DAG) that is a self-certifying append-only log of updates to the CRO.
2. A CRO can have many **replicas**, each maintaining a **hash graph replica**.
3. The set of updates in a hash graph replica that have no parents is called the hash graph's **frontier**.
4. Each replica has a **public key** identifying itself that is known by every other replica.

The next section looks at multiple possible definitions of CRO snapshotting.

## 4. A good CRO snapshot?
Assume for a given CRO, we have replicas $$R_1$$, $$R_2$$, $$R_3$$. At a particular instant in earth time, their hash graphs look like the following.

$$R_1$$'s hash graph:
```
a <- b
  <- c <- d
```

$$R_2$$'s hash graph:
```
a <- b
  <- c
```

$$R_3$$'s hash graph:
```
a <- b <- e
  <- c
```

What works as a snapshot of this CRO? For example, this probably works well:

```
a <- b
  <- c
```

This represents a "greatest common denominator" among all 3 hash graphs.

But how about this:

```
a <- b <- e
  <- c <- d
```

This represents the "merging of all hash graphs" and actually captures the most information of this CRO. Note that in-flight messages are already captured in these hash graphs, because the replica that generates an update needs to append the update to its local hash graph before sending the update to the network.

Both the "greatest common denominator" approach and the "merging of all replicas" approach require the notion of *all* - the need to know all hash graph replicas at the moment of snapshotting.

The notion of *all* can be very problematic due to network conditions and dynamic membership. We wish to define CRO snapshotting without involving the notion of *all*.

It is easy to see that the problem of snapshotting is equivalent to the problem of consensus: we want the replicas of a CRO to agree on a singular hash graph that represents the CRO's history at an instant of time.

## 5. Problem definition
Thus we are ready to define the problem of CRO snapshotting:

*A CRO snapshot is a single hash graph that is agreed upon by the replicas of the CRO.*

## 6. A leader-based algorithm
Designate one of the replica of a CRO as leader. The leader is responsible for maintaining a tick (an monotonic increasing counter) and for broadcasting heartbeats per each tick to all other replicas. When receiving a heartbeat, a replica responds by sending its hash graph replica back. The leader collects these hash graph replicas and single-sidedly determines a snapshot for each tick. After snapshot determination, the leader broadcasts the snapshot out.

The message complexity of the above algorithm is $$N$$ (heartbeat broadcast) + $$N$$ (replica response) + $$N$$ (snapshot broadcast) = $$3N$$.

If we combine the heartbeat broadcast of tick $$t+1$$ with the snapshot broadcast of tick $$t$$, the message complexity goes down to $$2N$$.

The leader can exercise a stopping condition - how many hash graph replicas would count as justifying a snapshot (since waiting for *all* is problematic) - as well as a cutoff condition - what is the maximum wait time between heartbeat broadcast and snapshot determination (this is to accommodate network conditions).

## 7. Issues
The issues with the algorithm above include:
1. For the best (fairest) latency performance, the leader replica needs to locate at roughly the geometric center of the p2p network connecting the replicas.
2. If the leader goes down, we need a mechanism to detect that and designate a new leader.
3. The leader needs to be particularly trustworthy because a CRO snapshot is really useful (see section 1).

These issues all emanate from the fact that the algorithm is leader-based. It would be helpful to eliminate the need for a leader.

## 8. Threshold Logical Clock

This section introduces Threshold Logical Clocks ([TLC](https://arxiv.org/pdf/1907.07010)), a primitive for decentralized pace-making. The next section will show how TLC can be used to build a leaderless consensus protocol in a layered fashion.

The following is a high level description of the procedure run by each process in a distributed system that operates a TLC:
1. **State**: Every process maintains a current tick number `T`, a set of signatures `SigSet`, and a set of unique witnessed messages `WitSet`.
2. **Parameter**: The TLC is parametrized by two thresholds: ack threshold `T_ACK`, and witness message threshold `T_WIT`.
3. **The `ADV` message**: When the process is ready to advance its tick: increment `T`, clear both `SigSet` and `CerSet`, create an tick-advancement message `ADV` containing some payload and `T`, and broadcast the message. This payload can be empty, or used by the upperlayer protocol or application for purposes such as consensus.
4. **The `ACK` message**: Any process that receives an `ADV` message would check that its signature is valid and that its tick value equals the process's own tick value. If good, the process replies an `ACK` message to the sender of the `ADV` message.
5. **The `WIT` message**: When the process that sent an `ADV` message collects more than `T_ACK` amount of `ACK`s for that `ADV`, the `ADV` message is considered *witnessed*. The process would then broadcast a `WIT` message, which contains the original `ADV` as well as an aggregated signature produced from the signatures of the `ACK`s - collected in `SigSet`.
6. **Ready to advance the tick**: When a process receives  more than `T_WIT` amount of unique `WIT` messages - collected in `WitSet` - of the current tick, the process is ready to advance its tick.

The procedure above is expressed in pseudocode form below:
```
parameters:
    T_ACK: int
    T_WIT: int

state:
    T: int
    SigSet: Set<Signature>
    WitSet: Set<WIT>

advanceTick():
    T <- T+1
    Clear SigSet
    Clear WitMsgSet
    Get payload P from upperlayer protocol
    Create message ADV = (p=P,t=T)
    Broadcast ADV

on Receive Msg from Sender S:
    if Msg.t > T:
        advanceTick() // virally catch up
    else if Msg.t == T:
        match Msg.type:
            ADV:
                Create message ACK=(p=ADV,sig=sign(ADV))
                Reply ACK to S
            ACK:
                Validate signature ACK.sig
                Insert ACK.sig to SigSet
                if SigSet.size > T_ACK:
                    broadcastWitnessedMessage()
            WIT:
                Validate aggregated signature WIT.sig
                Insert WIT to WitSet
                if WitSet.size > T_WIT:
                    advanceTick()

broadcastWitnessedMessage():
    Produce aggregated signature SigAgg from SigSet
    Create message WIT = (p=ADV,t=T,sig=SigAgg)
    Broadcast WIT
```

The message complexity of a single TLC tick is $$N$$ (`ADV` broadcast) + $$N$$ (`ACK` reply) + $$N$$ (`WIT` broadcast) multiplied by $$N$$ (symmetric protocol) = $$3N^2$$.

## 9. The TLC "contract"
In short, the TLC primitive offers the following contract to its upperlayer protocol:
1. I advance an integer tick (`T`) for you by communicating with other nodes in the network. I guarantee you my tick always stays in the vicinity of the ticks of other nodes, and that my tick catches up as fast as it can given network conditions.
2. At tick advancement, I notify you so that you may provide me with a message (payload `P` in `advanceTick()`) to be incorporated in my communication with other nodes for the purpose of tick advancement.
3. For every past tick, I provide you with a set of witnessed messages (`WitSet`). Each of these witnessed messages is guaranteed to have been seen by a threshold amount of nodes (`T_ACK`) in the network. Additionally, the size of this set for each past tick is always above a threshold amount (`T_WIT`).

In this way, TLC is useful for implementing consensus. The payload `P` can be used for submitting proposals to be decided by consensus. Finality (concering "what proposal has been decided?") can be observed in `WitSet` of the past ticks. The next section provides more details.

## 10. TLC-based consensus
A consensus algorithm can be built on top of the TLC primitive.

#### 3 ticks as 1 consensus round
Use 3 TLC ticks as 1 consensus round.

Notationally, denote the consensus round number as `r`, and TLC tick number as `t`. A round consists of 3 ticks: `3r`, `3r+1`, `3r+2`. The following steps take place:

1. In `t=3r`, the message payload of a process equals the process's own proposal.
2. In `t=3r+1`, the message payload equals the process's `WitSet` in `t=3r`.
3. In `t=3r+2`, each process simply broadcasts their `WitSet` in `t=3r+1`.

When `t=3r+2` completes, each process yields a list of proposals (denoted as $$\vec{P}$$) for the current round `r`. Each of these proposals ($$P$$) is either "not witnessed" (denoted as $$W^0$$), "singly witnessed" (denoted as $$W^1$$), or "doubly witnessed" (denoted as $$W^2$$).

#### Deciding
Deciding if consensus has been reached on a proposal this round, at each process locally, is carried out in the following steps:
1. Rank $$\vec{P}$$ by some deterministic algorithm consistent across all processes. For example, rank the list by sender ID lexicographically.
2. Take the first proposal in the ranked list. Denote the proposal as $$P_0$$.
3. If $$P_0$$ is not $$W^2$$ => this process can not decide in this current consensus round; nevertheless, in the next round this process's proposal will be built on top of $$P_0$$.
4. Otherwise, $$P_0$$ is decided. In the next round, this process's proposal will be built on top of $$P_0$$.

#### "Compatibility" between subsequent proposals
Vidigueira's notion of compatibility means that for process $$i$$ in round $$r$$, if a proposal $$P_r$$ is decided, then $$i$$'s proposal for $$r+1$$ or $$P_{r+1}$$ must build on top of $$P_r$$. In other words, $$P_{r+1}$$ causally depends on $$P_r$$. This is equivalent to in blockchains a new block is always proposed on top of the main (e.g. longest) chain.

## 11. A leaderless snapshot algorithm
A CRO snapshot algorithm using TLC-based consensus is leaderless. Following both the TLC procedure in section 6 and the consensus procedure in section 8, the following two points are added to achieve CRO (hashgraph) snapshotting:

1. The proposal at `t=3r` of each consensus round `r` is the frontier of the process's hash graph replica. The frontier *covers* (or *summarizes*) the entire graph by hash links. In other words, the frontier of a hashgraph is its digest. Therefore, proposing a frontier is equivalent to proposing the entire graph.
2. To achieve compatibility between subsequent proposals, if a process learns that a frontier $$F_r$$ was decided in round $$r$$, the process's proposal for `r+1` must *cover* $$F_r$$.

## 12. Issues
#### Consistency property
A lemma is provided in [Vidigueira 2019]:

***Lemma 5.1**
If any node sees a message m as K-certified by round R, all honest nodes will see m as (K-1)-certified by round R+1.*

However, what degree of consistency guarantee is provided by this snapshot algorithm requires further study. The basic question of interest to answer here is:

*If an honest node $$i$$ decides proposal $$P$$ (finality) in round $$r$$, is there a lowerbound over $$\delta$$ such that another random honest node $$j$$ decides $$P$$ in round $$r+\delta$$ with probability $$p=1$$? Or, is this finality always probabilistic i.e. $$j$$ decides $$P$$ in round $$r+\delta$$ with $$p=1-\epsilon$$, and $$\epsilon$$ decreases (how fast?) over increasing $$\delta$$?*

#### Latency
The latency of a single consensus round can be high because of the need to repeatedly collect signatures, aggregate them and re-broadcast them. Recent work [Babel et. al. 2024] proposes a solution to safely minimize the latency of a consensus round. We can check their approach.

#### Byzantine fault tolerance
This is a big topic. There are many kinds of Byzantine faults. I expect this topic to be addressed in many future TIPs. Some thoughts here:
1. Hashgraphs already provide tamper-proof property to causal ordering. Changing the agreed upon past (e.g. consensus has decided $$P$$ at round $$r$$; a malicious process proposes a new proposal at round $$r+1$$ which builds on / claims that it was $$P'$$ that was decided at round $$r$$) is prevented by the self-certifying property of hashgraphs.
2. Equivocation is a form of Byzantine fault. Equivocation can be defined as a process making more than one proposal during `t=3r` of a consensus round, particularly submitting different proposals to different peer processes. This shall be detectable and reportable, but details (detection, blocking, and recovery from contaminated hashgraphs) need to be worked out.
3. The deterministic ranking procedure for $$\vec{P}$$ in the first step of *deciding* (see section 8) can be exploited by malicious processes to increase the chance of their proposal getting decided as consensus. If the ranking procedure needs randomness to be fair, [drand](https://drand.love/) may be used. This needs further research to devise a clean algorithm without exploding complexity.

## 13. Next
The primary next step is to create a software prototype of the TLC-driven CRO snapshot algorithm above, validate that it has the desired property (e.g. consistency and termination), and study its behavior and performance.

## 14. Reference

[Chandy & Lamport 1985] Distributed Snapshots: Determining Global States of Distributed Systems. [link](https://lamport.azurewebsites.net/pubs/chandy.pdf)

[Vidigueira 2019] Threshold Logical Clocks. [link](https://www.epfl.ch/labs/dedis/wp-content/uploads/2020/01/report-2019-1-Manuel_Vidigueira.pdf)

[Ford 2019] Threshold Logical Clocks for Asynchronous Distributed Coordination and Consensus. [link](https://arxiv.org/pdf/1907.07010)

[Babel et. al. 2024] Mysticeti: Reaching the Limits of Latency with Uncertified DAGs. [link](https://arxiv.org/pdf/2310.14821)